{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLsGFMux8cctc5/odqfQfA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HEa40WweKUGP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b529d64b-84d8-4197-e81b-1e2a47ec5ab3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training point-forecast model (MSE)...\n","Epoch 1/12  train_loss=0.1489 val_rmse=0.2759 val_mae=0.2168 time=275.8s\n","Epoch 2/12  train_loss=0.0878 val_rmse=0.2574 val_mae=0.2034 time=266.4s\n","Epoch 3/12  train_loss=0.0834 val_rmse=0.2636 val_mae=0.2076 time=268.7s\n","Epoch 4/12  train_loss=0.0808 val_rmse=0.2626 val_mae=0.2071 time=277.2s\n"]}],"source":["# forecast_uq.py\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from typing import Tuple, List\n","from copy import deepcopy\n","import time\n","\n","# ---------------------------\n","# Utils / Data generation\n","# ---------------------------\n","def generate_sine_dataset(n_series=1000, length=300, seed=0):\n","    rng = np.random.RandomState(seed)\n","    data = []\n","    for i in range(n_series):\n","        freq = rng.uniform(0.01, 0.1)\n","        phase = rng.uniform(0, 2*np.pi)\n","        trend = rng.uniform(-0.01, 0.01) * np.arange(length)\n","        noise = rng.normal(scale=0.2, size=length)\n","        series = np.sin(np.arange(length) * freq + phase) + trend + noise\n","        data.append(series.astype(np.float32))\n","    return np.stack(data)  # shape (n_series, length)\n","\n","class TimeSeriesDataset(Dataset):\n","    \"\"\"\n","    Sliding-window dataset.\n","    Given series shape (n_series, series_len) produce pairs (encoder_seq, decoder_target)\n","    encoder_len = input window length\n","    horizon = how many steps to forecast\n","    \"\"\"\n","    def __init__(self, series: np.ndarray, encoder_len: int=48, horizon: int=12):\n","        # series: (n_series, series_len)\n","        self.series = series\n","        self.enc_len = encoder_len\n","        self.horizon = horizon\n","        self.items = []\n","        n_series, series_len = series.shape\n","        for i in range(n_series):\n","            for t in range(series_len - encoder_len - horizon + 1):\n","                self.items.append((i, t))\n","    def __len__(self):\n","        return len(self.items)\n","    def __getitem__(self, idx):\n","        i, t = self.items[idx]\n","        seq = self.series[i, t:t+self.enc_len]           # encoder input\n","        target = self.series[i, t+self.enc_len:t+self.enc_len+self.horizon]  # future horizon\n","        return torch.from_numpy(seq).unsqueeze(-1), torch.from_numpy(target).unsqueeze(-1)\n","        # shapes: (enc_len, 1), (horizon, 1)\n","\n","# ---------------------------\n","# Model: Transformer Encoder -> MLP head\n","# Simple temporal Transformer that outputs multi-step forecast.\n","# Includes dropout layers we can use for MC dropout.\n","# ---------------------------\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(pos * div_term)\n","        pe[:, 1::2] = torch.cos(pos * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n","    def forward(self, x):\n","        # x shape: (batch, seq_len, d_model)\n","        x = x + self.pe[:, :x.size(1), :].to(x.device)\n","        return x\n","\n","class TransformerForecaster(nn.Module):\n","    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=3,\n","                 dim_feedforward=128, dropout=0.1, horizon=12, use_quantiles: List[float]=None):\n","        \"\"\"\n","        If use_quantiles is None, model predicts mean value for each horizon step.\n","        If use_quantiles is a list like [0.1, 0.5, 0.9], the head predicts len(use_quantiles) values per horizon.\n","        \"\"\"\n","        super().__init__()\n","        self.d_model = d_model\n","        self.input_proj = nn.Linear(input_dim, d_model)\n","        self.pos_enc = PositionalEncoding(d_model)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n","                                                   dim_feedforward=dim_feedforward,\n","                                                   dropout=dropout, activation='gelu')\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","        self.dropout = nn.Dropout(dropout)  # this dropout will be used for MC-dropout\n","        self.horizon = horizon\n","        self.use_quantiles = use_quantiles\n","        out_feats = horizon * (len(use_quantiles) if use_quantiles else 1)\n","        self.head = nn.Sequential(\n","            nn.Linear(d_model, d_model//2),\n","            nn.GELU(),\n","            nn.Linear(d_model//2, out_feats)\n","        )\n","\n","    def forward(self, x):\n","        # x: (batch, seq_len, 1)\n","        x = self.input_proj(x) * math.sqrt(self.d_model)\n","        x = self.pos_enc(x)\n","        # transformer expects (seq_len, batch, d_model)\n","        xf = self.transformer(x.transpose(0,1)).transpose(0,1)  # (batch, seq_len, d_model)\n","        # Pool across time (use last token or mean pool)\n","        pooled = xf.mean(dim=1)  # (batch, d_model)\n","        pooled = self.dropout(pooled)\n","        out = self.head(pooled)  # (batch, out_feats)\n","        out = out.view(out.size(0), self.horizon, -1)  # (batch, horizon, feats_per_step)\n","        # If not quantiles, feats_per_step==1\n","        return out  # (batch, horizon, q) where q is 1 or num_quantiles\n","\n","# ---------------------------\n","# Losses: MSE and Pinball (quantile) loss\n","# ---------------------------\n","def mse_loss(pred, target):\n","    return ((pred - target)**2).mean()\n","\n","def pinball_loss(pred, target, q):\n","    # pred, target: (...), q scalar or tensor in [0,1]\n","    diff = target - pred\n","    return torch.max(q*diff, (q-1.0)*diff).mean()\n","\n","def multi_quantile_loss(preds, target, quantiles: List[float]):\n","    # preds: (batch, horizon, len(quantiles))\n","    loss = 0.0\n","    for i, q in enumerate(quantiles):\n","        loss = loss + pinball_loss(preds[:,:,i], target.squeeze(-1), q)\n","    return loss / len(quantiles)\n","\n","# ---------------------------\n","# Training / Eval helpers\n","# ---------------------------\n","def train_one_epoch(model, loader, optimizer, device, use_quantiles=None):\n","    model.train()\n","    total_loss = 0.0\n","    n = 0\n","    for x, y in loader:\n","        x = x.to(device)     # (B, enc_len, 1)\n","        y = y.to(device)     # (B, horizon, 1)\n","        optimizer.zero_grad()\n","        out = model(x)       # (B, horizon, q)\n","        if use_quantiles is None:\n","            pred = out.squeeze(-1)  # (B, horizon)\n","            loss = mse_loss(pred, y.squeeze(-1))\n","        else:\n","            loss = multi_quantile_loss(out, y, use_quantiles)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * x.size(0)\n","        n += x.size(0)\n","    return total_loss / n\n","\n","def evaluate(model, loader, device, use_quantiles=None):\n","    model.eval()\n","    total_mse = 0.0\n","    total_mae = 0.0\n","    n = 0\n","    preds = []\n","    trues = []\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device)\n","            out = model(x)\n","            if use_quantiles is None:\n","                pred = out.squeeze(-1)\n","                total_mse += ((pred - y.squeeze(-1))**2).sum().item()\n","                total_mae += (pred - y.squeeze(-1)).abs().sum().item()\n","                preds.append(pred.cpu().numpy())\n","            else:\n","                # pick median quantile index\n","                q_idx = use_quantiles.index(0.5) if 0.5 in use_quantiles else len(use_quantiles)//2\n","                pred = out[:,:,q_idx]\n","                total_mse += ((pred - y.squeeze(-1))**2).sum().item()\n","                total_mae += (pred - y.squeeze(-1)).abs().sum().item()\n","                preds.append(pred.cpu().numpy())\n","            trues.append(y.squeeze(-1).cpu().numpy())\n","            n += y.numel()\n","    rmse = math.sqrt(total_mse / n)\n","    mae = total_mae / n\n","    preds = np.concatenate(preds, axis=0)\n","    trues = np.concatenate(trues, axis=0)\n","    return rmse, mae, preds, trues\n","\n","# ---------------------------\n","# MC Dropout prediction\n","# ---------------------------\n","def mc_dropout_predict(model, x, device, mc_samples=50):\n","    \"\"\"\n","    Run model forward mc_samples times with dropout active.\n","    Returns: mean_pred (batch, horizon), std_pred (batch, horizon)\n","    \"\"\"\n","    model.train()  # IMPORTANT: leave dropout active, but there are no BN layers here\n","    preds = []\n","    with torch.no_grad():\n","        for _ in range(mc_samples):\n","            out = model(x.to(device))  # (B, horizon, q)\n","            out = out.squeeze(-1).cpu().numpy()\n","            preds.append(out)\n","    preds = np.stack(preds, axis=0)  # (mc, batch, horizon)\n","    mean = preds.mean(axis=0)\n","    std = preds.std(axis=0)\n","    model.eval()\n","    return mean, std\n","\n","# ---------------------------\n","# Example: train & demonstrate\n","# ---------------------------\n","def main():\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # generate data\n","    series = generate_sine_dataset(n_series=500, length=300, seed=42)\n","    # train / val / test split along series\n","    n = series.shape[0]\n","    train_series = series[:380]\n","    val_series = series[380:440]\n","    test_series = series[440:]\n","    encoder_len = 48\n","    horizon = 12\n","\n","    train_ds = TimeSeriesDataset(train_series, encoder_len, horizon)\n","    val_ds = TimeSeriesDataset(val_series, encoder_len, horizon)\n","    test_ds = TimeSeriesDataset(test_series, encoder_len, horizon)\n","\n","    batch_size = 64\n","    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n","    val_loader = DataLoader(val_ds, batch_size=batch_size)\n","    test_loader = DataLoader(test_ds, batch_size=batch_size)\n","\n","    # ---------------------------\n","    # 1) Baseline: point forecast (MSE) with dropout (for MC)\n","    # ---------------------------\n","    model = TransformerForecaster(input_dim=1, d_model=64, nhead=4, num_layers=3,\n","                                  dim_feedforward=128, dropout=0.2, horizon=horizon,\n","                                  use_quantiles=None)\n","    model.to(device)\n","    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n","    epochs = 12\n","    print(\"Training point-forecast model (MSE)...\")\n","    best_val = 1e9\n","    for ep in range(epochs):\n","        t0 = time.time()\n","        tr_loss = train_one_epoch(model, train_loader, opt, device, use_quantiles=None)\n","        rmse_val, mae_val, _, _ = evaluate(model, val_loader, device, use_quantiles=None)\n","        if rmse_val < best_val:\n","            best_val = rmse_val\n","            best_model_state = deepcopy(model.state_dict())\n","        print(f\"Epoch {ep+1}/{epochs}  train_loss={tr_loss:.4f} val_rmse={rmse_val:.4f} val_mae={mae_val:.4f} time={time.time()-t0:.1f}s\")\n","    model.load_state_dict(best_model_state)\n","\n","    # Evaluate on test set\n","    rmse_test, mae_test, preds_pt, trues_pt = evaluate(model, test_loader, device, use_quantiles=None)\n","    print(f\"Point model test RMSE={rmse_test:.4f}, MAE={mae_test:.4f}\")\n","\n","    # ---------------------------\n","    # 2) MC Dropout for uncertainty\n","    # ---------------------------\n","    # Take a batch from test_loader and run MC dropout\n","    x_batch, y_batch = next(iter(test_loader))\n","    mean_mc, std_mc = mc_dropout_predict(model, x_batch, device, mc_samples=100)\n","    # Example: compute 95% interval using Gaussian approx\n","    lower_95 = mean_mc - 1.96 * std_mc\n","    upper_95 = mean_mc + 1.96 * std_mc\n","    # compute empirical coverage on this batch (how many true values fall into the interval)\n","    y_np = y_batch.squeeze(-1).numpy()\n","    coverage_95 = ((y_np >= lower_95) & (y_np <= upper_95)).mean()\n","    print(f\"MC Dropout batch coverage ~95% interval: {coverage_95*100:.2f}% (on single batch)\")\n","\n","    # ---------------------------\n","    # 3) Quantile regression model (predict multiple quantiles directly)\n","    # ---------------------------\n","    quantiles = [0.1, 0.5, 0.9]\n","    q_model = TransformerForecaster(input_dim=1, d_model=64, nhead=4, num_layers=3,\n","                                    dim_feedforward=128, dropout=0.2, horizon=horizon,\n","                                    use_quantiles=quantiles)\n","    q_model.to(device)\n","    q_opt = torch.optim.Adam(q_model.parameters(), lr=1e-3)\n","    epochs_q = 12\n","    print(\"Training quantile regression model...\")\n","    best_val = 1e9\n","    for ep in range(epochs_q):\n","        tr_loss = train_one_epoch(q_model, train_loader, q_opt, device, use_quantiles=quantiles)\n","        rmse_val, mae_val, _, _ = evaluate(q_model, val_loader, device, use_quantiles=quantiles)\n","        if rmse_val < best_val:\n","            best_val = rmse_val\n","            best_model_state = deepcopy(q_model.state_dict())\n","        print(f\"Epoch {ep+1}/{epochs_q}  q_train_loss={tr_loss:.4f} val_rmse={rmse_val:.4f}\")\n","    q_model.load_state_dict(best_model_state)\n","\n","    # Evaluate quantile coverage on a batch\n","    q_model.eval()\n","    xb, yb = next(iter(test_loader))\n","    with torch.no_grad():\n","        out = q_model(xb.to(device)).cpu().numpy()  # (B, horizon, q)\n","    # pick quantiles index\n","    q_lower = 0  # 0.1\n","    q_median = 1 # 0.5\n","    q_upper = 2  # 0.9\n","    lower_q = out[:,:,q_lower]\n","    med_q = out[:,:,q_median]\n","    upper_q = out[:,:,q_upper]\n","    y_np = yb.squeeze(-1).numpy()\n","    cov_q = ((y_np >= lower_q) & (y_np <= upper_q)).mean()\n","    print(f\"Quantile model empirical coverage 10-90 interval: {cov_q*100:.2f}% (on single batch)\")\n","\n","    # ---------------------------\n","    # 4) Deep Ensembles: train multiple point-forecast models and aggregate\n","    # ---------------------------\n","    n_ensembles = 3\n","    ensemble_models = []\n","    print(\"Training deep ensemble of point models...\")\n","    for m in range(n_ensembles):\n","        m_model = TransformerForecaster(input_dim=1, d_model=64, nhead=4, num_layers=3,\n","                                       dim_feedforward=128, dropout=0.2, horizon=horizon,\n","                                       use_quantiles=None)\n","        m_model.to(device)\n","        m_opt = torch.optim.Adam(m_model.parameters(), lr=1e-3)\n","        # quick train (few epochs) for demo\n","        for ep in range(6):\n","            train_one_epoch(m_model, train_loader, m_opt, device, use_quantiles=None)\n","        ensemble_models.append(deepcopy(m_model).cpu())\n","    # Aggregate predictions on batch\n","    xb, yb = next(iter(test_loader))\n","    preds_ens = []\n","    for m in ensemble_models:\n","        m.eval()\n","        with torch.no_grad():\n","            p = m(xb.to(device)).squeeze(-1).cpu().numpy()  # (B, horizon)\n","            preds_ens.append(p)\n","    preds_ens = np.stack(preds_ens, axis=0)  # (n_ens, B, horizon)\n","    ens_mean = preds_ens.mean(axis=0)\n","    ens_std = preds_ens.std(axis=0)\n","    # Example coverage using mean +/- 1.96*std\n","    cov_ens = ((yb.squeeze(-1).numpy() >= ens_mean - 1.96*ens_std) & (yb.squeeze(-1).numpy() <= ens_mean + 1.96*ens_std)).mean()\n","    print(f\"Ensemble empirical coverage (approx 95%): {cov_ens*100:.2f}% (on single batch)\")\n","\n","    # ---------------------------\n","    # Final test RMSE for quantile & point models\n","    # ---------------------------\n","    rmse_point, mae_point, _, _ = evaluate(model, test_loader, device, use_quantiles=None)\n","    rmse_q, mae_q, _, _ = evaluate(q_model, test_loader, device, use_quantiles=quantiles)\n","    print(f\"Final test RMSE: point_model={rmse_point:.4f}, quantile_model(median)={rmse_q:.4f}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}